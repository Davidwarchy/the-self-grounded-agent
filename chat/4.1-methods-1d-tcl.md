# Data 
- Let's have an experiment
- Random walk, one a one dimensional integer line 
- Collect 0.5 million entries
# Training 
- We then do some sort of temporal contrastive learning on the data 
- Let's have an embedding dimension of 2 
- Readings that apppear closer should have the same embeddings 
- Use a simple MLP
- Print loss on every epoch 
# Visualization 
- Show how the 1d readings are spread across the 2d space... ie, show mapping 

# 
Experiment done here: https://colab.research.google.com/drive/14gTsJBZGMJnmCPqac1Z9_dmXEmE0CiTc#scrollTo=fvDoe4tn0EU_
- points continuous in the reading are continuous in the embedding dimension 
- - what various activation functions (relu, sigmoid, tanh) do is that they change the shapes of the curves from the readings dimension to the embedding dimension. For example relu gives two straight line with a hinge join, tanh gives curves, sigmoid gives S-shaped vurves
